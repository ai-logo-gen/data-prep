{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283f9f60",
   "metadata": {},
   "source": [
    "# Balanced Image Extraction from Amazing Logos V4\n",
    "\n",
    "This notebook extracts a balanced sample of images from the Amazing Logos V4 dataset:\n",
    "- Loads the HuggingFace dataset from input/amazing_logos_v4/\n",
    "- Loads metadata9.csv for category information\n",
    "- Samples metadata with equal distribution across categories\n",
    "- Extracts images efficiently using index-based access\n",
    "- Configurable total number of images to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb73cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Total images to extract: 2,000\n",
      "  Image size: (512, 512)\n",
      "  Output format: PNG\n",
      "  Output name: balanced_sample_2k_512x512\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "TOTAL_IMAGES_TO_EXTRACT = 2000  # Total number of images to extract\n",
    "IMAGE_SIZE = (512, 512)  # Size to save images\n",
    "OUTPUT_FORMAT = 'PNG'  # Image format to save\n",
    "OUTPUT_NAME = 'balanced_sample_2k_512x512' # Output folder and file name prefix\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Total images to extract: {TOTAL_IMAGES_TO_EXTRACT:,}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}\")\n",
    "print(f\"  Output format: {OUTPUT_FORMAT}\")\n",
    "print(f\"  Output name: {OUTPUT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2eba57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paul-\\miniconda3\\envs\\py311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset: ..\\..\\input\\amazing_logos_v4\\train\n",
      "Metadata file: ..\\..\\output\\amazing_logos_v4\\data\\amazing_logos_v4_image_prep\\metadata10.csv\n",
      "Output images: ..\\..\\output\\amazing_logos_v4\\images\\balanced_sample_2k_512x512\n",
      "Output metadata: ..\\..\\output\\amazing_logos_v4\\data\\amazing_logos_v4_image_prep\\balanced_sample_2k_512x512_metadata.csv\n",
      "✅ All required paths exist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Setup paths\n",
    "input_dataset_path = Path('../../input/amazing_logos_v4/train')\n",
    "metadata_path = Path('../../output/amazing_logos_v4/data/amazing_logos_v4_image_prep/metadata10.csv')\n",
    "output_images_path = Path(f'../../output/amazing_logos_v4/images/{OUTPUT_NAME}')\n",
    "output_metadata_path = Path(f'../../output/amazing_logos_v4/data/amazing_logos_v4_image_prep/{OUTPUT_NAME}_metadata.csv')\n",
    "\n",
    "# Create output directory\n",
    "output_images_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input dataset: {input_dataset_path}\")\n",
    "print(f\"Metadata file: {metadata_path}\")\n",
    "print(f\"Output images: {output_images_path}\")\n",
    "print(f\"Output metadata: {output_metadata_path}\")\n",
    "\n",
    "# Check if paths exist\n",
    "if not input_dataset_path.exists():\n",
    "    print(f\"❌ Input dataset path does not exist: {input_dataset_path}\")\n",
    "if not metadata_path.exists():\n",
    "    print(f\"❌ Metadata file does not exist: {metadata_path}\")\n",
    "else:\n",
    "    print(\"✅ All required paths exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "✅ Loaded metadata: 177,224 rows\n",
      "\n",
      "Found 10 unique categories\n",
      "Top 10 categories:\n",
      "   1. other: 65,033 (36.7%)\n",
      "   2. tech: 17,631 (9.9%)\n",
      "   3. retail_hospitality: 17,380 (9.8%)\n",
      "   4. entertainment_sports_media: 15,427 (8.7%)\n",
      "   5. food_beverage: 12,835 (7.2%)\n",
      "   6. health: 12,277 (6.9%)\n",
      "   7. professional_financial_legal: 11,931 (6.7%)\n",
      "   8. real_estate_construction: 11,022 (6.2%)\n",
      "   9. manufacturing_transport: 8,508 (4.8%)\n",
      "  10. education: 5,180 (2.9%)\n",
      "✅ Loaded metadata: 177,224 rows\n",
      "\n",
      "Found 10 unique categories\n",
      "Top 10 categories:\n",
      "   1. other: 65,033 (36.7%)\n",
      "   2. tech: 17,631 (9.9%)\n",
      "   3. retail_hospitality: 17,380 (9.8%)\n",
      "   4. entertainment_sports_media: 15,427 (8.7%)\n",
      "   5. food_beverage: 12,835 (7.2%)\n",
      "   6. health: 12,277 (6.9%)\n",
      "   7. professional_financial_legal: 11,931 (6.7%)\n",
      "   8. real_estate_construction: 11,022 (6.2%)\n",
      "   9. manufacturing_transport: 8,508 (4.8%)\n",
      "  10. education: 5,180 (2.9%)\n"
     ]
    }
   ],
   "source": [
    "# Load metadata to get category information\n",
    "print(\"Loading metadata...\")\n",
    "try:\n",
    "    # Load metadata in chunks due to large size\n",
    "    metadata_chunks = []\n",
    "    chunk_size = 50000\n",
    "    \n",
    "    for chunk in pd.read_csv(metadata_path, chunksize=chunk_size):\n",
    "        metadata_chunks.append(chunk[['id', 'category_main']])  # Only load relevant columns\n",
    "    \n",
    "    metadata_df = pd.concat(metadata_chunks, ignore_index=True)\n",
    "    print(f\"✅ Loaded metadata: {len(metadata_df):,} rows\")\n",
    "    \n",
    "    # Show category distribution\n",
    "    category_counts = metadata_df['category_main'].value_counts()\n",
    "    print(f\"\\nFound {len(category_counts)} unique categories\")\n",
    "    print(f\"Top 10 categories:\")\n",
    "    for i, (cat, count) in enumerate(category_counts.head(10).items(), 1):\n",
    "        percentage = (count / len(metadata_df)) * 100\n",
    "        print(f\"  {i:2d}. {cat}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading metadata: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49962dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced sampling strategy:\n",
      "  Total categories: 10\n",
      "  Base images per category: 200\n",
      "  Remainder to distribute: 0\n",
      "  retail_hospitality: sampled 200/17380\n",
      "  manufacturing_transport: sampled 200/8508\n",
      "  other: sampled 200/65033\n",
      "  entertainment_sports_media: sampled 200/15427\n",
      "  health: sampled 200/12277\n",
      "  education: sampled 200/5180\n",
      "  food_beverage: sampled 200/12835\n",
      "  real_estate_construction: sampled 200/11022\n",
      "  tech: sampled 200/17631\n",
      "  professional_financial_legal: sampled 200/11931\n",
      "\n",
      "Total sampled metadata: 2,000 rows\n",
      "\n",
      "Sampled distribution:\n",
      "  retail_hospitality: 200\n",
      "  manufacturing_transport: 200\n",
      "  other: 200\n",
      "  entertainment_sports_media: 200\n",
      "  health: 200\n",
      "  education: 200\n",
      "  food_beverage: 200\n",
      "  real_estate_construction: 200\n",
      "  tech: 200\n",
      "  professional_financial_legal: 200\n"
     ]
    }
   ],
   "source": [
    "# Sample metadata with balanced distribution across categories\n",
    "unique_categories = metadata_df['category_main'].unique()\n",
    "num_categories = len(unique_categories)\n",
    "target_per_category = TOTAL_IMAGES_TO_EXTRACT // num_categories\n",
    "remainder = TOTAL_IMAGES_TO_EXTRACT % num_categories\n",
    "\n",
    "print(f\"Balanced sampling strategy:\")\n",
    "print(f\"  Total categories: {num_categories}\")\n",
    "print(f\"  Base images per category: {target_per_category}\")\n",
    "print(f\"  Remainder to distribute: {remainder}\")\n",
    "\n",
    "# Sample from metadata directly\n",
    "sampled_metadata = []\n",
    "for i, category in enumerate(unique_categories):\n",
    "    target = target_per_category + (1 if i < remainder else 0)\n",
    "    \n",
    "    # Get all rows for this category\n",
    "    category_rows = metadata_df[metadata_df['category_main'] == category]\n",
    "    available = len(category_rows)\n",
    "    actual_target = min(target, available)\n",
    "    \n",
    "    # Sample randomly from this category\n",
    "    if actual_target > 0:\n",
    "        sampled_rows = category_rows.sample(n=actual_target, random_state=42)\n",
    "        sampled_metadata.append(sampled_rows)\n",
    "        print(f\"  {category}: sampled {actual_target}/{available}\")\n",
    "\n",
    "# Combine all sampled metadata\n",
    "if sampled_metadata:\n",
    "    sampled_df = pd.concat(sampled_metadata, ignore_index=True)\n",
    "    print(f\"\\nTotal sampled metadata: {len(sampled_df):,} rows\")\n",
    "    \n",
    "    # Show distribution\n",
    "    sampled_counts = sampled_df['category_main'].value_counts()\n",
    "    print(f\"\\nSampled distribution:\")\n",
    "    for category, count in sampled_counts.head(10).items():\n",
    "        print(f\"  {category}: {count}\")\n",
    "else:\n",
    "    print(\"❌ No metadata could be sampled!\")\n",
    "    sampled_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f338d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 arrow files:\n",
      "  1. data-00000-of-00029.arrow\n",
      "  2. data-00001-of-00029.arrow\n",
      "  3. data-00002-of-00029.arrow\n",
      "  4. data-00003-of-00029.arrow\n",
      "  5. data-00004-of-00029.arrow\n",
      "  ... and 24 more\n"
     ]
    }
   ],
   "source": [
    "# Get list of arrow files\n",
    "arrow_files = list(input_dataset_path.glob('data-*.arrow'))\n",
    "arrow_files.sort()\n",
    "\n",
    "print(f\"Found {len(arrow_files)} arrow files:\")\n",
    "for i, file in enumerate(arrow_files[:5], 1):\n",
    "    print(f\"  {i}. {file.name}\")\n",
    "if len(arrow_files) > 5:\n",
    "    print(f\"  ... and {len(arrow_files) - 5} more\")\n",
    "\n",
    "# Update the variable name for consistency\n",
    "parquet_files = arrow_files  # Keep the same variable name to avoid breaking other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6f28c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining the structure of the first dataset file...\n",
      "Dataset features: {'image': Image(mode=None, decode=True), 'text': Value('string')}\n",
      "Dataset length: 13699\n",
      "\n",
      "First example:\n",
      "Keys: ['image', 'text']\n",
      "  image: <class 'PIL.PngImagePlugin.PngImageFile'> - PIL Image with size (512, 512)\n",
      "  text: Simple elegant logo for Mandarin Oriental, Fan Hong kong Lines Paper, Hospitality, successful vibe, ...\n",
      "\n",
      "Trying to load the full dataset from directory...\n",
      "Keys: ['image', 'text']\n",
      "  image: <class 'PIL.PngImagePlugin.PngImageFile'> - PIL Image with size (512, 512)\n",
      "  text: Simple elegant logo for Mandarin Oriental, Fan Hong kong Lines Paper, Hospitality, successful vibe, ...\n",
      "\n",
      "Trying to load the full dataset from directory...\n",
      "Full dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 397251\n",
      "    })\n",
      "})\n",
      "Train split: 397251 examples\n",
      "Features: {'image': Image(mode=None, decode=True), 'text': Value('string')}\n",
      "\n",
      "==================================================\n",
      "EXAMINING SAMPLED METADATA ID FORMAT\n",
      "==================================================\n",
      "Sample of IDs from metadata:\n",
      "   1. amazing_logo_v4294142\n",
      "   2. amazing_logo_v4201237\n",
      "   3. amazing_logo_v4164292\n",
      "   4. amazing_logo_v4280843\n",
      "   5. amazing_logo_v4007417\n",
      "   6. amazing_logo_v4023863\n",
      "   7. amazing_logo_v4266833\n",
      "   8. amazing_logo_v4184365\n",
      "   9. amazing_logo_v4238862\n",
      "  10. amazing_logo_v4163305\n",
      "\n",
      "Analyzing ID format to extract indices...\n",
      "  amazing_logo_v4294142 -> all numbers: ['4294142']\n",
      "    -> extracted index (after v4): 294142\n",
      "  amazing_logo_v4201237 -> all numbers: ['4201237']\n",
      "    -> extracted index (after v4): 201237\n",
      "  amazing_logo_v4164292 -> all numbers: ['4164292']\n",
      "    -> extracted index (after v4): 164292\n",
      "  amazing_logo_v4280843 -> all numbers: ['4280843']\n",
      "    -> extracted index (after v4): 280843\n",
      "  amazing_logo_v4007417 -> all numbers: ['4007417']\n",
      "    -> extracted index (after v4): 7417\n",
      "  amazing_logo_v4023863 -> all numbers: ['4023863']\n",
      "    -> extracted index (after v4): 23863\n",
      "  amazing_logo_v4266833 -> all numbers: ['4266833']\n",
      "    -> extracted index (after v4): 266833\n",
      "  amazing_logo_v4184365 -> all numbers: ['4184365']\n",
      "    -> extracted index (after v4): 184365\n",
      "  amazing_logo_v4238862 -> all numbers: ['4238862']\n",
      "    -> extracted index (after v4): 238862\n",
      "  amazing_logo_v4163305 -> all numbers: ['4163305']\n",
      "    -> extracted index (after v4): 163305\n",
      "\n",
      "Corrected approach: Extract numbers after 'v4' prefix, ignoring version number.\n",
      "Full dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 397251\n",
      "    })\n",
      "})\n",
      "Train split: 397251 examples\n",
      "Features: {'image': Image(mode=None, decode=True), 'text': Value('string')}\n",
      "\n",
      "==================================================\n",
      "EXAMINING SAMPLED METADATA ID FORMAT\n",
      "==================================================\n",
      "Sample of IDs from metadata:\n",
      "   1. amazing_logo_v4294142\n",
      "   2. amazing_logo_v4201237\n",
      "   3. amazing_logo_v4164292\n",
      "   4. amazing_logo_v4280843\n",
      "   5. amazing_logo_v4007417\n",
      "   6. amazing_logo_v4023863\n",
      "   7. amazing_logo_v4266833\n",
      "   8. amazing_logo_v4184365\n",
      "   9. amazing_logo_v4238862\n",
      "  10. amazing_logo_v4163305\n",
      "\n",
      "Analyzing ID format to extract indices...\n",
      "  amazing_logo_v4294142 -> all numbers: ['4294142']\n",
      "    -> extracted index (after v4): 294142\n",
      "  amazing_logo_v4201237 -> all numbers: ['4201237']\n",
      "    -> extracted index (after v4): 201237\n",
      "  amazing_logo_v4164292 -> all numbers: ['4164292']\n",
      "    -> extracted index (after v4): 164292\n",
      "  amazing_logo_v4280843 -> all numbers: ['4280843']\n",
      "    -> extracted index (after v4): 280843\n",
      "  amazing_logo_v4007417 -> all numbers: ['4007417']\n",
      "    -> extracted index (after v4): 7417\n",
      "  amazing_logo_v4023863 -> all numbers: ['4023863']\n",
      "    -> extracted index (after v4): 23863\n",
      "  amazing_logo_v4266833 -> all numbers: ['4266833']\n",
      "    -> extracted index (after v4): 266833\n",
      "  amazing_logo_v4184365 -> all numbers: ['4184365']\n",
      "    -> extracted index (after v4): 184365\n",
      "  amazing_logo_v4238862 -> all numbers: ['4238862']\n",
      "    -> extracted index (after v4): 238862\n",
      "  amazing_logo_v4163305 -> all numbers: ['4163305']\n",
      "    -> extracted index (after v4): 163305\n",
      "\n",
      "Corrected approach: Extract numbers after 'v4' prefix, ignoring version number.\n"
     ]
    }
   ],
   "source": [
    "# Debug: Examine the structure and ID format\n",
    "if arrow_files:\n",
    "    print(\"Examining the structure of the first dataset file...\")\n",
    "    try:\n",
    "        # Try loading with datasets library\n",
    "        dataset = Dataset.from_file(str(arrow_files[0]))\n",
    "        print(f\"Dataset features: {dataset.features}\")\n",
    "        print(f\"Dataset length: {len(dataset)}\")\n",
    "        \n",
    "        if len(dataset) > 0:\n",
    "            print(\"\\nFirst example:\")\n",
    "            first_example = dataset[0]\n",
    "            print(\"Keys:\", list(first_example.keys()))\n",
    "            \n",
    "            for key, value in first_example.items():\n",
    "                if key == 'image':\n",
    "                    print(f\"  {key}: {type(value)} - PIL Image with size {value.size if hasattr(value, 'size') else 'unknown'}\")\n",
    "                elif isinstance(value, str) and len(value) > 100:\n",
    "                    print(f\"  {key}: {value[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                    \n",
    "        # Check if we can load the full dataset\n",
    "        print(f\"\\nTrying to load the full dataset from directory...\")\n",
    "        try:\n",
    "            from datasets import load_from_disk\n",
    "            full_dataset = load_from_disk(str(input_dataset_path.parent))\n",
    "            print(f\"Full dataset loaded: {full_dataset}\")\n",
    "            if 'train' in full_dataset:\n",
    "                train_data = full_dataset['train']\n",
    "                print(f\"Train split: {len(train_data)} examples\")\n",
    "                print(f\"Features: {train_data.features}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load full dataset: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error examining dataset file: {e}\")\n",
    "\n",
    "# Examine sampled metadata ID format\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"EXAMINING SAMPLED METADATA ID FORMAT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(sampled_df) > 0:\n",
    "    print(f\"Sample of IDs from metadata:\")\n",
    "    sample_ids = sampled_df['id'].head(10).tolist()\n",
    "    for i, id_val in enumerate(sample_ids, 1):\n",
    "        print(f\"  {i:2d}. {id_val}\")\n",
    "    \n",
    "    # Try to extract index pattern\n",
    "    print(f\"\\nAnalyzing ID format to extract indices...\")\n",
    "    id_patterns = []\n",
    "    for id_val in sample_ids:\n",
    "        if isinstance(id_val, str):\n",
    "            # Try to extract numeric parts\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', id_val)\n",
    "            if numbers:\n",
    "                print(f\"  {id_val} -> all numbers: {numbers}\")\n",
    "                \n",
    "                # Try the corrected pattern for v4\n",
    "                match = re.search(r'amazing_logo_v4(\\d+)', id_val)\n",
    "                if match:\n",
    "                    extracted_idx = int(match.group(1))\n",
    "                    print(f\"    -> extracted index (after v4): {extracted_idx}\")\n",
    "                else:\n",
    "                    # Alternative pattern\n",
    "                    match = re.search(r'v4(\\d+)', id_val)\n",
    "                    if match:\n",
    "                        extracted_idx = int(match.group(1))\n",
    "                        print(f\"    -> extracted index (v4 pattern): {extracted_idx}\")\n",
    "                    else:\n",
    "                        # Filter out version number\n",
    "                        filtered_numbers = [int(n) for n in numbers if int(n) != 4]\n",
    "                        if filtered_numbers:\n",
    "                            extracted_idx = max(filtered_numbers)\n",
    "                            print(f\"    -> extracted index (largest non-4): {extracted_idx}\")\n",
    "                        else:\n",
    "                            print(f\"    -> no valid index found\")\n",
    "                \n",
    "                id_patterns.append(numbers)\n",
    "            else:\n",
    "                print(f\"  {id_val} -> no numbers found\")\n",
    "        else:\n",
    "            print(f\"  {id_val} -> non-string ID\")\n",
    "    \n",
    "    print(f\"\\nCorrected approach: Extract numbers after 'v4' prefix, ignoring version number.\")\n",
    "else:\n",
    "    print(\"No sampled metadata available for ID analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fcce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting efficient extraction of 2000 images...\n",
      "Loading full dataset...\n",
      "✅ Loaded dataset with 397251 examples\n",
      "✅ Loaded dataset with 397251 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:   6%|▌         | 112/2000 [00:01<00:20, 93.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 100 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  11%|█         | 211/2000 [00:02<00:20, 85.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 200 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  16%|█▌        | 315/2000 [00:03<00:18, 90.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 300 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  21%|██        | 419/2000 [00:04<00:17, 92.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 400 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  26%|██▌       | 512/2000 [00:05<00:17, 84.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 500 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  30%|███       | 608/2000 [00:06<00:14, 96.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 600 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  36%|███▌      | 712/2000 [00:07<00:13, 96.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 700 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  41%|████      | 815/2000 [00:08<00:13, 88.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 800 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  46%|████▌     | 916/2000 [00:10<00:13, 82.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 900 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  51%|█████     | 1013/2000 [00:11<00:11, 88.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  56%|█████▌    | 1111/2000 [00:12<00:10, 87.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1100 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  61%|██████    | 1220/2000 [00:13<00:08, 93.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1200 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  66%|██████▌   | 1317/2000 [00:14<00:06, 98.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1300 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  70%|███████   | 1409/2000 [00:15<00:06, 91.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1400 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  76%|███████▌  | 1512/2000 [00:16<00:05, 92.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1500 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  81%|████████  | 1613/2000 [00:17<00:04, 93.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1600 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  86%|████████▌ | 1713/2000 [00:18<00:02, 96.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1700 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  91%|█████████ | 1811/2000 [00:19<00:02, 87.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1800 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images:  96%|█████████▌| 1913/2000 [00:21<00:00, 90.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 1900 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 100%|██████████| 2000/2000 [00:22<00:00, 90.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 2000 images...\n",
      "\n",
      "✅ Extraction completed!\n",
      "  Successfully extracted: 2,000\n",
      "  Failed extractions: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Efficient index-based image extraction\n",
    "extracted_images = []\n",
    "failed_extractions = []\n",
    "\n",
    "if len(sampled_df) == 0:\n",
    "    print(\"❌ No sampled metadata available for extraction!\")\n",
    "else:\n",
    "    print(f\"Starting efficient extraction of {len(sampled_df)} images...\")\n",
    "    \n",
    "    # Load the full dataset once\n",
    "    try:\n",
    "        from datasets import load_from_disk\n",
    "        print(\"Loading full dataset...\")\n",
    "        full_dataset = load_from_disk(str(input_dataset_path.parent))\n",
    "        \n",
    "        if 'train' in full_dataset:\n",
    "            dataset = full_dataset['train']\n",
    "            print(f\"✅ Loaded dataset with {len(dataset)} examples\")\n",
    "            \n",
    "            # Process each sampled metadata row\n",
    "            for idx, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc=\"Extracting images\"):\n",
    "                image_id = row['id']\n",
    "                category = row['category_main']\n",
    "                \n",
    "                try:\n",
    "                    # Extract dataset index from ID\n",
    "                    dataset_idx = None\n",
    "                    \n",
    "                    if isinstance(image_id, str):\n",
    "                        # Try different patterns to extract index\n",
    "                        import re\n",
    "                        \n",
    "                        # Pattern 1: amazing_logo_v4XXXXXXX (digits after v4)\n",
    "                        match = re.search(r'amazing_logo_v4(\\d+)', image_id)\n",
    "                        if match:\n",
    "                            dataset_idx = int(match.group(1))\n",
    "                        else:\n",
    "                            # Pattern 2: Look for version pattern and extract number after it\n",
    "                            match = re.search(r'v4(\\d+)', image_id)\n",
    "                            if match:\n",
    "                                dataset_idx = int(match.group(1))\n",
    "                            else:\n",
    "                                # Pattern 3: amazing_logo_v4_XXXXX_XXXXXX (file_row format)\n",
    "                                match = re.search(r'amazing_logo_v4_(\\d{5})_(\\d{6})', image_id)\n",
    "                                if match:\n",
    "                                    file_idx = int(match.group(1))\n",
    "                                    row_idx = int(match.group(2))\n",
    "                                    # Estimate dataset index (rough approximation)\n",
    "                                    # This assumes each file has roughly the same number of rows\n",
    "                                    estimated_rows_per_file = len(dataset) // len(arrow_files)\n",
    "                                    dataset_idx = file_idx * estimated_rows_per_file + row_idx\n",
    "                                else:\n",
    "                                    # Pattern 4: Extract all numbers and use the last/largest one\n",
    "                                    numbers = re.findall(r'\\d+', image_id)\n",
    "                                    if numbers:\n",
    "                                        # Filter out the version number (4) and use the actual index\n",
    "                                        filtered_numbers = [int(n) for n in numbers if int(n) != 4]\n",
    "                                        if filtered_numbers:\n",
    "                                            dataset_idx = max(filtered_numbers)  # Use largest number as index\n",
    "                                        else:\n",
    "                                            dataset_idx = int(numbers[-1])  # Fallback to last number\n",
    "                    \n",
    "                    elif isinstance(image_id, (int, float)):\n",
    "                        dataset_idx = int(image_id)\n",
    "                    \n",
    "                    if dataset_idx is None or dataset_idx >= len(dataset):\n",
    "                        failed_extractions.append({\n",
    "                            'id': image_id,\n",
    "                            'reason': f'Could not extract valid index (extracted: {dataset_idx}, max: {len(dataset)-1})'\n",
    "                        })\n",
    "                        continue\n",
    "                    \n",
    "                    # Get the image directly by index\n",
    "                    item = dataset[dataset_idx]\n",
    "                    \n",
    "                    if 'image' not in item:\n",
    "                        failed_extractions.append({\n",
    "                            'id': image_id,\n",
    "                            'reason': 'No image field in dataset item'\n",
    "                        })\n",
    "                        continue\n",
    "                    \n",
    "                    image_data = item['image']\n",
    "                    \n",
    "                    # Handle PIL Image (most common in HuggingFace datasets)\n",
    "                    if hasattr(image_data, 'save'):  # PIL Image\n",
    "                        image = image_data\n",
    "                    elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
    "                        image_bytes = image_data['bytes']\n",
    "                        image = Image.open(io.BytesIO(image_bytes))\n",
    "                    elif isinstance(image_data, bytes):\n",
    "                        image = Image.open(io.BytesIO(image_data))\n",
    "                    else:\n",
    "                        failed_extractions.append({\n",
    "                            'id': image_id,\n",
    "                            'reason': f'Unknown image format: {type(image_data)}'\n",
    "                        })\n",
    "                        continue\n",
    "                    \n",
    "                    # Resize image\n",
    "                    image = image.convert('RGB')\n",
    "                    image = image.resize(IMAGE_SIZE, Image.Resampling.LANCZOS)\n",
    "                    \n",
    "                    # Save image\n",
    "                    image_filename = f\"{image_id}.{OUTPUT_FORMAT.lower()}\"\n",
    "                    image_path = output_images_path / image_filename\n",
    "                    image.save(image_path, OUTPUT_FORMAT)\n",
    "                    \n",
    "                    # Record successful extraction\n",
    "                    extracted_images.append({\n",
    "                        'id': image_id,\n",
    "                        'category_main': category,\n",
    "                        'filename': image_filename,\n",
    "                        'dataset_index': dataset_idx\n",
    "                    })\n",
    "                    \n",
    "                    # Progress update every 100 images\n",
    "                    if len(extracted_images) % 100 == 0:\n",
    "                        print(f\"  Extracted {len(extracted_images)} images...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_extractions.append({\n",
    "                        'id': image_id,\n",
    "                        'reason': f'Error: {str(e)}'\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            print(f\"\\n✅ Extraction completed!\")\n",
    "            print(f\"  Successfully extracted: {len(extracted_images):,}\")\n",
    "            print(f\"  Failed extractions: {len(failed_extractions):,}\")\n",
    "            \n",
    "            if failed_extractions and len(failed_extractions) <= 10:\n",
    "                print(f\"\\nFirst few failures:\")\n",
    "                for i, failure in enumerate(failed_extractions[:5], 1):\n",
    "                    print(f\"  {i}. {failure['id']}: {failure['reason']}\")\n",
    "                    \n",
    "        else:\n",
    "            print(\"❌ No 'train' split found in dataset\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load dataset: {e}\")\n",
    "        print(\"This approach requires the full dataset to be loadable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5991bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction results by category:\n",
      "Category                       Target   Extracted  %     \n",
      "------------------------------------------------------------\n",
      "education                      200      200        100.0%\n",
      "entertainment_sports_media     200      200        100.0%\n",
      "food_beverage                  200      200        100.0%\n",
      "health                         200      200        100.0%\n",
      "manufacturing_transport        200      200        100.0%\n",
      "other                          200      200        100.0%\n",
      "professional_financial_legal   200      200        100.0%\n",
      "real_estate_construction       200      200        100.0%\n",
      "retail_hospitality             200      200        100.0%\n",
      "tech                           200      200        100.0%\n",
      "------------------------------------------------------------\n",
      "TOTAL                          2000     2000       100.0%\n"
     ]
    }
   ],
   "source": [
    "# Show extraction results by category\n",
    "if extracted_images:\n",
    "    extracted_df = pd.DataFrame(extracted_images)\n",
    "    category_counts_extracted = extracted_df['category_main'].value_counts()\n",
    "    \n",
    "    print(\"\\nExtraction results by category:\")\n",
    "    print(f\"{'Category':<30} {'Target':<8} {'Extracted':<10} {'%':<6}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    sampled_counts = sampled_df['category_main'].value_counts()\n",
    "    for category in sorted(sampled_counts.index):\n",
    "        target = sampled_counts[category]\n",
    "        extracted = category_counts_extracted.get(category, 0)\n",
    "        percentage = (extracted / target * 100) if target > 0 else 0\n",
    "        print(f\"{category[:30]:<30} {target:<8} {extracted:<10} {percentage:>5.1f}%\")\n",
    "\n",
    "    total_target = len(sampled_df)\n",
    "    total_extracted = len(extracted_images)\n",
    "    overall_percentage = (total_extracted / total_target * 100) if total_target > 0 else 0\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'TOTAL':<30} {total_target:<8} {total_extracted:<10} {overall_percentage:>5.1f}%\")\n",
    "else:\n",
    "    print(\"❌ No images were extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373585fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved extraction metadata to: ..\\..\\output\\amazing_logos_v4\\data\\amazing_logos_v4_image_prep\\balanced_sample_2k_512x512_metadata.csv\n",
      "\n",
      "Sample of extracted images:\n",
      "                      id       category_main                   filename  \\\n",
      "0  amazing_logo_v4294142  retail_hospitality  amazing_logo_v4294142.png   \n",
      "1  amazing_logo_v4201237  retail_hospitality  amazing_logo_v4201237.png   \n",
      "2  amazing_logo_v4164292  retail_hospitality  amazing_logo_v4164292.png   \n",
      "3  amazing_logo_v4280843  retail_hospitality  amazing_logo_v4280843.png   \n",
      "4  amazing_logo_v4007417  retail_hospitality  amazing_logo_v4007417.png   \n",
      "\n",
      "   dataset_index  \n",
      "0         294142  \n",
      "1         201237  \n",
      "2         164292  \n",
      "3         280843  \n",
      "4           7417  \n",
      "\n",
      "✅ Saved extraction summary to: ..\\..\\output\\amazing_logos_v4\\images\\extraction_summary.json\n",
      "\n",
      "Extraction Summary:\n",
      "  Images sampled from metadata: 2,000\n",
      "  Images successfully extracted: 2,000\n",
      "  Extraction success rate: 100.0%\n",
      "  Categories: 10\n",
      "  Failed extractions: 0\n"
     ]
    }
   ],
   "source": [
    "# Save extraction metadata and summary\n",
    "if extracted_images:\n",
    "    extracted_df = pd.DataFrame(extracted_images)\n",
    "    extracted_df.to_csv(output_metadata_path, index=False)\n",
    "    print(f\"\\n✅ Saved extraction metadata to: {output_metadata_path}\")\n",
    "    \n",
    "    # Show sample of extracted data\n",
    "    print(f\"\\nSample of extracted images:\")\n",
    "    print(extracted_df.head())\n",
    "    \n",
    "    # Save summary statistics\n",
    "    category_distribution = extracted_df['category_main'].value_counts().to_dict()\n",
    "    \n",
    "    summary_stats = {\n",
    "        'total_images_extracted': len(extracted_images),\n",
    "        'total_images_target': TOTAL_IMAGES_TO_EXTRACT,\n",
    "        'total_images_sampled': len(sampled_df),\n",
    "        'extraction_percentage': (len(extracted_images) / len(sampled_df) * 100) if len(sampled_df) > 0 else 0,\n",
    "        'categories_processed': len(category_distribution),\n",
    "        'failed_extractions': len(failed_extractions),\n",
    "        'image_size': IMAGE_SIZE,\n",
    "        'output_format': OUTPUT_FORMAT,\n",
    "        'category_distribution': category_distribution\n",
    "    }\n",
    "    \n",
    "    summary_path = output_images_path.parent / 'extraction_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ Saved extraction summary to: {summary_path}\")\n",
    "    print(f\"\\nExtraction Summary:\")\n",
    "    print(f\"  Images sampled from metadata: {summary_stats['total_images_sampled']:,}\")\n",
    "    print(f\"  Images successfully extracted: {summary_stats['total_images_extracted']:,}\")\n",
    "    print(f\"  Extraction success rate: {summary_stats['extraction_percentage']:.1f}%\")\n",
    "    print(f\"  Categories: {summary_stats['categories_processed']}\")\n",
    "    print(f\"  Failed extractions: {summary_stats['failed_extractions']}\")\n",
    "    \n",
    "    # Save failed extractions for debugging if any\n",
    "    if failed_extractions:\n",
    "        failed_df = pd.DataFrame(failed_extractions)\n",
    "        failed_path = output_images_path.parent / 'failed_extractions.csv'\n",
    "        failed_df.to_csv(failed_path, index=False)\n",
    "        print(f\"  Failed extractions saved to: {failed_path}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n❌ No images were extracted!\")\n",
    "    if failed_extractions:\n",
    "        print(f\"Total failures: {len(failed_extractions)}\")\n",
    "        print(\"Sample failures:\")\n",
    "        for i, failure in enumerate(failed_extractions[:5], 1):\n",
    "            print(f\"  {i}. {failure['id']}: {failure['reason']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
