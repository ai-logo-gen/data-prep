{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283f9f60",
   "metadata": {},
   "source": [
    "# Complete Image Extraction from Amazing Logos V4\n",
    "\n",
    "This notebook extracts all images present in metadata9.csv from the Amazing Logos V4 dataset:\n",
    "- Loads the HuggingFace dataset from input/amazing_logos_v4/\n",
    "- Loads metadata9.csv for complete image list\n",
    "- Extracts all images that are present in the metadata (after cleanup)\n",
    "- Saves all images to total_after_cleanup folder\n",
    "- No sampling - extracts every image from the cleaned metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb73cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Extracting ALL images from metadata9.csv\n",
      "  Image size: (256, 256)\n",
      "  Output format: PNG\n",
      "  No sampling - complete extraction based on cleaned metadata\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "IMAGE_SIZE = (256, 256)  # Size to save images\n",
    "OUTPUT_FORMAT = 'PNG'  # Image format to save\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Extracting ALL images from metadata9.csv\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}\")\n",
    "print(f\"  Output format: {OUTPUT_FORMAT}\")\n",
    "print(f\"  No sampling - complete extraction based on cleaned metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eba57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset: ..\\..\\input\\amazing_logos_v4\\train\n",
      "Metadata file: ..\\..\\output\\amazing_logos_v4\\data\\amazing_logos_v4_cleanup\\metadata9.csv\n",
      "Output images: ..\\..\\output\\amazing_logos_v4\\images\\total_after_cleanup\n",
      "Output metadata: ..\\..\\output\\amazing_logos_v4\\data\\total_after_cleanup_metadata.csv\n",
      "‚úÖ All required paths exist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Setup paths\n",
    "input_dataset_path = Path('../../input/amazing_logos_v4/train')\n",
    "metadata_path = Path('../../output/amazing_logos_v4/data/amazing_logos_v4_cleanup/metadata9.csv')\n",
    "output_images_path = Path('../../output/amazing_logos_v4/images/total_after_cleanup')\n",
    "output_metadata_path = Path('../../output/amazing_logos_v4/data/amazing_logos_v4_image_prep/total_after_cleanup_metadata.csv')\n",
    "\n",
    "# Create output directory\n",
    "output_images_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input dataset: {input_dataset_path}\")\n",
    "print(f\"Metadata file: {metadata_path}\")\n",
    "print(f\"Output images: {output_images_path}\")\n",
    "print(f\"Output metadata: {output_metadata_path}\")\n",
    "\n",
    "# Check if paths exist\n",
    "if not input_dataset_path.exists():\n",
    "    print(f\"‚ùå Input dataset path does not exist: {input_dataset_path}\")\n",
    "if not metadata_path.exists():\n",
    "    print(f\"‚ùå Metadata file does not exist: {metadata_path}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required paths exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete metadata...\n",
      "‚úÖ Loaded metadata: 352,154 total images to extract\n",
      "\n",
      "Found 109 unique categories\n",
      "Top 10 categories:\n",
      "   1. unclassified: 73,954 (21.0%)\n",
      "   2. sports_recreation: 11,383 (3.2%)\n",
      "   3. restaurant_dining: 10,963 (3.1%)\n",
      "   4. real_estate_residential: 10,825 (3.1%)\n",
      "   5. healthcare_general: 10,534 (3.0%)\n",
      "   6. design_creative: 10,366 (2.9%)\n",
      "   7. nonprofit_charity: 10,134 (2.9%)\n",
      "   8. fashion_apparel: 9,174 (2.6%)\n",
      "   9. education_k12: 8,572 (2.4%)\n",
      "  10. music_industry: 7,318 (2.1%)\n",
      "\n",
      "Total images to extract: 352,154\n",
      "‚úÖ Loaded metadata: 352,154 total images to extract\n",
      "\n",
      "Found 109 unique categories\n",
      "Top 10 categories:\n",
      "   1. unclassified: 73,954 (21.0%)\n",
      "   2. sports_recreation: 11,383 (3.2%)\n",
      "   3. restaurant_dining: 10,963 (3.1%)\n",
      "   4. real_estate_residential: 10,825 (3.1%)\n",
      "   5. healthcare_general: 10,534 (3.0%)\n",
      "   6. design_creative: 10,366 (2.9%)\n",
      "   7. nonprofit_charity: 10,134 (2.9%)\n",
      "   8. fashion_apparel: 9,174 (2.6%)\n",
      "   9. education_k12: 8,572 (2.4%)\n",
      "  10. music_industry: 7,318 (2.1%)\n",
      "\n",
      "Total images to extract: 352,154\n"
     ]
    }
   ],
   "source": [
    "# Load complete metadata for all images\n",
    "print(\"Loading complete metadata...\")\n",
    "try:\n",
    "    # Load full metadata (only id and category columns for efficiency)\n",
    "    metadata_df = pd.read_csv(metadata_path, usecols=['id', 'category'])\n",
    "    print(f\"‚úÖ Loaded metadata: {len(metadata_df):,} total images to extract\")\n",
    "    \n",
    "    # Show category distribution\n",
    "    category_counts = metadata_df['category'].value_counts()\n",
    "    print(f\"\\nFound {len(category_counts)} unique categories\")\n",
    "    print(f\"Top 10 categories:\")\n",
    "    for i, (cat, count) in enumerate(category_counts.head(10).items(), 1):\n",
    "        percentage = (count / len(metadata_df)) * 100\n",
    "        print(f\"  {i:2d}. {cat}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTotal images to extract: {len(metadata_df):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading metadata: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49962dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction strategy: COMPLETE\n",
      "  Total images in metadata: 352,154\n",
      "  All images will be extracted (no sampling)\n",
      "\n",
      "Distribution of all images by category:\n",
      "   1. unclassified: 73,954 (21.0%)\n",
      "   2. sports_recreation: 11,383 (3.2%)\n",
      "   3. restaurant_dining: 10,963 (3.1%)\n",
      "   4. real_estate_residential: 10,825 (3.1%)\n",
      "   5. healthcare_general: 10,534 (3.0%)\n",
      "   6. design_creative: 10,366 (2.9%)\n",
      "   7. nonprofit_charity: 10,134 (2.9%)\n",
      "   8. fashion_apparel: 9,174 (2.6%)\n",
      "   9. education_k12: 8,572 (2.4%)\n",
      "  10. music_industry: 7,318 (2.1%)\n",
      "  11. marketing_advertising: 6,806 (1.9%)\n",
      "  12. retail_general: 6,503 (1.8%)\n",
      "  13. home_improvement: 6,200 (1.8%)\n",
      "  14. entertainment_venues: 6,089 (1.7%)\n",
      "  15. chemical_materials: 5,501 (1.6%)\n",
      "  ... and 94 more categories: 157,832 (44.8%)\n",
      "\n",
      "Total images to process: 352,154\n"
     ]
    }
   ],
   "source": [
    "# Use all metadata - no sampling needed\n",
    "print(f\"Extraction strategy: COMPLETE\")\n",
    "print(f\"  Total images in metadata: {len(metadata_df):,}\")\n",
    "print(f\"  All images will be extracted (no sampling)\")\n",
    "\n",
    "# Use the complete metadata as-is\n",
    "sampled_df = metadata_df.copy()\n",
    "\n",
    "print(f\"\\nDistribution of all images by category:\")\n",
    "category_counts = sampled_df['category'].value_counts()\n",
    "for i, (category, count) in enumerate(category_counts.head(15).items(), 1):\n",
    "    percentage = (count / len(sampled_df)) * 100\n",
    "    print(f\"  {i:2d}. {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "if len(category_counts) > 15:\n",
    "    remaining = len(category_counts) - 15\n",
    "    remaining_count = category_counts.tail(remaining).sum()\n",
    "    remaining_percentage = (remaining_count / len(sampled_df)) * 100\n",
    "    print(f\"  ... and {remaining} more categories: {remaining_count:,} ({remaining_percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal images to process: {len(sampled_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f338d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 arrow files:\n",
      "  1. data-00000-of-00029.arrow\n",
      "  2. data-00001-of-00029.arrow\n",
      "  3. data-00002-of-00029.arrow\n",
      "  4. data-00003-of-00029.arrow\n",
      "  5. data-00004-of-00029.arrow\n",
      "  ... and 24 more\n"
     ]
    }
   ],
   "source": [
    "# Get list of arrow files\n",
    "arrow_files = list(input_dataset_path.glob('data-*.arrow'))\n",
    "arrow_files.sort()\n",
    "\n",
    "print(f\"Found {len(arrow_files)} arrow files:\")\n",
    "for i, file in enumerate(arrow_files[:5], 1):\n",
    "    print(f\"  {i}. {file.name}\")\n",
    "if len(arrow_files) > 5:\n",
    "    print(f\"  ... and {len(arrow_files) - 5} more\")\n",
    "\n",
    "# Update the variable name for consistency\n",
    "parquet_files = arrow_files  # Keep the same variable name to avoid breaking other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6f28c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining the structure of the first dataset file...\n",
      "Dataset features: {'image': Image(mode=None, decode=True, id=None), 'text': Value(dtype='string', id=None)}\n",
      "Dataset length: 13699\n",
      "\n",
      "First example:\n",
      "Keys: ['image', 'text']\n",
      "  image: <class 'PIL.PngImagePlugin.PngImageFile'> - PIL Image with size (512, 512)\n",
      "  text: Simple elegant logo for Mandarin Oriental, Fan Hong kong Lines Paper, Hospitality, successful vibe, ...\n",
      "\n",
      "Trying to load the full dataset from directory...\n",
      "Keys: ['image', 'text']\n",
      "  image: <class 'PIL.PngImagePlugin.PngImageFile'> - PIL Image with size (512, 512)\n",
      "  text: Simple elegant logo for Mandarin Oriental, Fan Hong kong Lines Paper, Hospitality, successful vibe, ...\n",
      "\n",
      "Trying to load the full dataset from directory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607c04e7c47748748c539f2c9e1b0dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 397251\n",
      "    })\n",
      "})\n",
      "Train split: 397251 examples\n",
      "Features: {'image': Image(mode=None, decode=True, id=None), 'text': Value(dtype='string', id=None)}\n",
      "\n",
      "==================================================\n",
      "EXAMINING SAMPLED METADATA ID FORMAT\n",
      "==================================================\n",
      "Sample of IDs from metadata:\n",
      "   1. amazing_logo_v4000000\n",
      "   2. amazing_logo_v4000001\n",
      "   3. amazing_logo_v4000003\n",
      "   4. amazing_logo_v4000004\n",
      "   5. amazing_logo_v4000005\n",
      "   6. amazing_logo_v4000006\n",
      "   7. amazing_logo_v4000007\n",
      "   8. amazing_logo_v4000008\n",
      "   9. amazing_logo_v4000009\n",
      "  10. amazing_logo_v4000010\n",
      "\n",
      "Analyzing ID format to extract indices...\n",
      "  amazing_logo_v4000000 -> all numbers: ['4000000']\n",
      "    -> extracted index (after v4): 0\n",
      "  amazing_logo_v4000001 -> all numbers: ['4000001']\n",
      "    -> extracted index (after v4): 1\n",
      "  amazing_logo_v4000003 -> all numbers: ['4000003']\n",
      "    -> extracted index (after v4): 3\n",
      "  amazing_logo_v4000004 -> all numbers: ['4000004']\n",
      "    -> extracted index (after v4): 4\n",
      "  amazing_logo_v4000005 -> all numbers: ['4000005']\n",
      "    -> extracted index (after v4): 5\n",
      "  amazing_logo_v4000006 -> all numbers: ['4000006']\n",
      "    -> extracted index (after v4): 6\n",
      "  amazing_logo_v4000007 -> all numbers: ['4000007']\n",
      "    -> extracted index (after v4): 7\n",
      "  amazing_logo_v4000008 -> all numbers: ['4000008']\n",
      "    -> extracted index (after v4): 8\n",
      "  amazing_logo_v4000009 -> all numbers: ['4000009']\n",
      "    -> extracted index (after v4): 9\n",
      "  amazing_logo_v4000010 -> all numbers: ['4000010']\n",
      "    -> extracted index (after v4): 10\n",
      "\n",
      "Corrected approach: Extract numbers after 'v4' prefix, ignoring version number.\n"
     ]
    }
   ],
   "source": [
    "# Debug: Examine the structure and ID format\n",
    "if arrow_files:\n",
    "    print(\"Examining the structure of the first dataset file...\")\n",
    "    try:\n",
    "        # Try loading with datasets library\n",
    "        dataset = Dataset.from_file(str(arrow_files[0]))\n",
    "        print(f\"Dataset features: {dataset.features}\")\n",
    "        print(f\"Dataset length: {len(dataset)}\")\n",
    "        \n",
    "        if len(dataset) > 0:\n",
    "            print(\"\\nFirst example:\")\n",
    "            first_example = dataset[0]\n",
    "            print(\"Keys:\", list(first_example.keys()))\n",
    "            \n",
    "            for key, value in first_example.items():\n",
    "                if key == 'image':\n",
    "                    print(f\"  {key}: {type(value)} - PIL Image with size {value.size if hasattr(value, 'size') else 'unknown'}\")\n",
    "                elif isinstance(value, str) and len(value) > 100:\n",
    "                    print(f\"  {key}: {value[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "                    \n",
    "        # Check if we can load the full dataset\n",
    "        print(f\"\\nTrying to load the full dataset from directory...\")\n",
    "        try:\n",
    "            from datasets import load_from_disk\n",
    "            full_dataset = load_from_disk(str(input_dataset_path.parent))\n",
    "            print(f\"Full dataset loaded: {full_dataset}\")\n",
    "            if 'train' in full_dataset:\n",
    "                train_data = full_dataset['train']\n",
    "                print(f\"Train split: {len(train_data)} examples\")\n",
    "                print(f\"Features: {train_data.features}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load full dataset: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error examining dataset file: {e}\")\n",
    "\n",
    "# Examine sampled metadata ID format\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"EXAMINING SAMPLED METADATA ID FORMAT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(sampled_df) > 0:\n",
    "    print(f\"Sample of IDs from metadata:\")\n",
    "    sample_ids = sampled_df['id'].head(10).tolist()\n",
    "    for i, id_val in enumerate(sample_ids, 1):\n",
    "        print(f\"  {i:2d}. {id_val}\")\n",
    "    \n",
    "    # Try to extract index pattern\n",
    "    print(f\"\\nAnalyzing ID format to extract indices...\")\n",
    "    id_patterns = []\n",
    "    for id_val in sample_ids:\n",
    "        if isinstance(id_val, str):\n",
    "            # Try to extract numeric parts\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', id_val)\n",
    "            if numbers:\n",
    "                print(f\"  {id_val} -> all numbers: {numbers}\")\n",
    "                \n",
    "                # Try the corrected pattern for v4\n",
    "                match = re.search(r'amazing_logo_v4(\\d+)', id_val)\n",
    "                if match:\n",
    "                    extracted_idx = int(match.group(1))\n",
    "                    print(f\"    -> extracted index (after v4): {extracted_idx}\")\n",
    "                else:\n",
    "                    # Alternative pattern\n",
    "                    match = re.search(r'v4(\\d+)', id_val)\n",
    "                    if match:\n",
    "                        extracted_idx = int(match.group(1))\n",
    "                        print(f\"    -> extracted index (v4 pattern): {extracted_idx}\")\n",
    "                    else:\n",
    "                        # Filter out version number\n",
    "                        filtered_numbers = [int(n) for n in numbers if int(n) != 4]\n",
    "                        if filtered_numbers:\n",
    "                            extracted_idx = max(filtered_numbers)\n",
    "                            print(f\"    -> extracted index (largest non-4): {extracted_idx}\")\n",
    "                        else:\n",
    "                            print(f\"    -> no valid index found\")\n",
    "                \n",
    "                id_patterns.append(numbers)\n",
    "            else:\n",
    "                print(f\"  {id_val} -> no numbers found\")\n",
    "        else:\n",
    "            print(f\"  {id_val} -> non-string ID\")\n",
    "    \n",
    "    print(f\"\\nCorrected approach: Extract numbers after 'v4' prefix, ignoring version number.\")\n",
    "else:\n",
    "    print(\"No sampled metadata available for ID analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fcce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting complete extraction of 352,154 images...\n",
      "Loading full dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be716d40a64748de952753297e48f916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded dataset with 397,251 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracted 352,000 images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352154/352154 [49:31<00:00, 118.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Complete extraction finished!\n",
      "  Successfully extracted: 352,154\n",
      "  Failed extractions: 0\n",
      "  Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete image extraction from metadata\n",
    "extracted_images = []\n",
    "failed_extractions = []\n",
    "\n",
    "if len(sampled_df) == 0:\n",
    "    print(\"‚ùå No metadata available for extraction!\")\n",
    "else:\n",
    "    print(f\"Starting complete extraction of {len(sampled_df):,} images...\")\n",
    "    \n",
    "    # Load the full dataset once\n",
    "    try:\n",
    "        from datasets import load_from_disk\n",
    "        print(\"Loading full dataset...\")\n",
    "        full_dataset = load_from_disk(str(input_dataset_path.parent))\n",
    "        \n",
    "        if 'train' in full_dataset:\n",
    "            dataset = full_dataset['train']\n",
    "            print(f\"‚úÖ Loaded dataset with {len(dataset):,} examples\")\n",
    "            \n",
    "            # Process each metadata row\n",
    "            with tqdm(total=len(sampled_df), desc=\"Extracting all images\") as pbar:\n",
    "                for idx, row in sampled_df.iterrows():\n",
    "                    image_id = row['id']\n",
    "                    category = row['category']\n",
    "                    \n",
    "                    try:\n",
    "                        # Extract dataset index from ID\n",
    "                        dataset_idx = None\n",
    "                        \n",
    "                        if isinstance(image_id, str):\n",
    "                            # Try different patterns to extract index\n",
    "                            import re\n",
    "                            \n",
    "                            # Pattern 1: amazing_logo_v4XXXXXXX (digits after v4)\n",
    "                            match = re.search(r'amazing_logo_v4(\\d+)', image_id)\n",
    "                            if match:\n",
    "                                dataset_idx = int(match.group(1))\n",
    "                            else:\n",
    "                                # Pattern 2: Look for version pattern and extract number after it\n",
    "                                match = re.search(r'v4(\\d+)', image_id)\n",
    "                                if match:\n",
    "                                    dataset_idx = int(match.group(1))\n",
    "                                else:\n",
    "                                    # Pattern 3: amazing_logo_v4_XXXXX_XXXXXX (file_row format)\n",
    "                                    match = re.search(r'amazing_logo_v4_(\\d{5})_(\\d{6})', image_id)\n",
    "                                    if match:\n",
    "                                        file_idx = int(match.group(1))\n",
    "                                        row_idx = int(match.group(2))\n",
    "                                        # Estimate dataset index (rough approximation)\n",
    "                                        estimated_rows_per_file = len(dataset) // len(arrow_files)\n",
    "                                        dataset_idx = file_idx * estimated_rows_per_file + row_idx\n",
    "                                    else:\n",
    "                                        # Pattern 4: Extract all numbers and use the last/largest one\n",
    "                                        numbers = re.findall(r'\\d+', image_id)\n",
    "                                        if numbers:\n",
    "                                            # Filter out the version number (4) and use the actual index\n",
    "                                            filtered_numbers = [int(n) for n in numbers if int(n) != 4]\n",
    "                                            if filtered_numbers:\n",
    "                                                dataset_idx = max(filtered_numbers)  # Use largest number as index\n",
    "                                            else:\n",
    "                                                dataset_idx = int(numbers[-1])  # Fallback to last number\n",
    "                        \n",
    "                        elif isinstance(image_id, (int, float)):\n",
    "                            dataset_idx = int(image_id)\n",
    "                        \n",
    "                        if dataset_idx is None or dataset_idx >= len(dataset):\n",
    "                            failed_extractions.append({\n",
    "                                'id': image_id,\n",
    "                                'reason': f'Could not extract valid index (extracted: {dataset_idx}, max: {len(dataset)-1})'\n",
    "                            })\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        # Get the image directly by index\n",
    "                        item = dataset[dataset_idx]\n",
    "                        \n",
    "                        if 'image' not in item:\n",
    "                            failed_extractions.append({\n",
    "                                'id': image_id,\n",
    "                                'reason': 'No image field in dataset item'\n",
    "                            })\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        image_data = item['image']\n",
    "                        \n",
    "                        # Handle PIL Image (most common in HuggingFace datasets)\n",
    "                        if hasattr(image_data, 'save'):  # PIL Image\n",
    "                            image = image_data\n",
    "                        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
    "                            image_bytes = image_data['bytes']\n",
    "                            image = Image.open(io.BytesIO(image_bytes))\n",
    "                        elif isinstance(image_data, bytes):\n",
    "                            image = Image.open(io.BytesIO(image_data))\n",
    "                        else:\n",
    "                            failed_extractions.append({\n",
    "                                'id': image_id,\n",
    "                                'reason': f'Unknown image format: {type(image_data)}'\n",
    "                            })\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        # Resize image\n",
    "                        image = image.convert('RGB')\n",
    "                        image = image.resize(IMAGE_SIZE, Image.Resampling.LANCZOS)\n",
    "                        \n",
    "                        # Save image\n",
    "                        image_filename = f\"{image_id}.{OUTPUT_FORMAT.lower()}\"\n",
    "                        image_path = output_images_path / image_filename\n",
    "                        image.save(image_path, OUTPUT_FORMAT)\n",
    "                        \n",
    "                        # Record successful extraction\n",
    "                        extracted_images.append({\n",
    "                            'id': image_id,\n",
    "                            'category': category,\n",
    "                            'filename': image_filename,\n",
    "                            'dataset_index': dataset_idx\n",
    "                        })\n",
    "                        \n",
    "                        # Update progress description every 1000 images\n",
    "                        if len(extracted_images) % 1000 == 0:\n",
    "                            pbar.set_description(f\"Extracted {len(extracted_images):,} images\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        failed_extractions.append({\n",
    "                            'id': image_id,\n",
    "                            'reason': f'Error: {str(e)}'\n",
    "                        })\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Complete extraction finished!\")\n",
    "            print(f\"  Successfully extracted: {len(extracted_images):,}\")\n",
    "            print(f\"  Failed extractions: {len(failed_extractions):,}\")\n",
    "            print(f\"  Success rate: {(len(extracted_images)/len(sampled_df)*100):.1f}%\")\n",
    "            \n",
    "            if failed_extractions and len(failed_extractions) <= 10:\n",
    "                print(f\"\\nFirst few failures:\")\n",
    "                for i, failure in enumerate(failed_extractions[:5], 1):\n",
    "                    print(f\"  {i}. {failure['id']}: {failure['reason']}\")\n",
    "                    \n",
    "        else:\n",
    "            print(\"‚ùå No 'train' split found in dataset\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not load dataset: {e}\")\n",
    "        print(\"This approach requires the full dataset to be loadable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5991bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete extraction results by category:\n",
      "Category                       In Metadata  Extracted  %     \n",
      "-----------------------------------------------------------------\n",
      "accounting_tax                 661          661        100.0%\n",
      "aerospace_defense              229          229        100.0%\n",
      "agriculture_farming            637          637        100.0%\n",
      "animal_services                744          744        100.0%\n",
      "arts_culture                   5195         5195       100.0%\n",
      "auto_services                  392          392        100.0%\n",
      "automotive_transport           2201         2201       100.0%\n",
      "aviation_services              734          734        100.0%\n",
      "bar_nightlife                  495          495        100.0%\n",
      "beauty_cosmetics               1511         1511       100.0%\n",
      "beverage_general               3916         3916       100.0%\n",
      "brewery_alcohol                4233         4233       100.0%\n",
      "cafe_coffee                    2911         2911       100.0%\n",
      "catering_events                2268         2268       100.0%\n",
      "chemical_materials             5501         5501       100.0%\n",
      "childcare_youth                1145         1145       100.0%\n",
      "cleaning_maintenance           265          265        100.0%\n",
      "community_social               1261         1261       100.0%\n",
      "construction_general           1441         1441       100.0%\n",
      "construction_home              573          573        100.0%\n",
      "construction_materials         627          627        100.0%\n",
      "construction_specialty         1412         1412       100.0%\n",
      "consulting_business            1127         1127       100.0%\n",
      "data_analytics                 296          296        100.0%\n",
      "dental_services                1466         1466       100.0%\n",
      "design_creative                10366        10366      100.0%\n",
      "ecommerce_online               934          934        100.0%\n",
      "education_k12                  8572         8572       100.0%\n",
      "educational_services           1098         1098       100.0%\n",
      "energy_utilities               2000         2000       100.0%\n",
      "entertainment_venues           6089         6089       100.0%\n",
      "environmental_services         1430         1430       100.0%\n",
      "event_services                 875          875        100.0%\n",
      "fashion_apparel                9174         9174       100.0%\n",
      "film_video                     3077         3077       100.0%\n",
      "financial_services             4645         4645       100.0%\n",
      "fintech_crypto                 1112         1112       100.0%\n",
      "fitness_health                 217          217        100.0%\n",
      "food_production                5161         5161       100.0%\n",
      "footwear_leather               482          482        100.0%\n",
      "forestry_lumber                489          489        100.0%\n",
      "funeral_services               182          182        100.0%\n",
      "gambling_betting               304          304        100.0%\n",
      "gaming_entertainment           2076         2076       100.0%\n",
      "government_public              2425         2425       100.0%\n",
      "grocery_retail                 871          871        100.0%\n",
      "health_wellness                346          346        100.0%\n",
      "healthcare_general             10534        10534      100.0%\n",
      "higher_education               1043         1043       100.0%\n",
      "home_goods                     2173         2173       100.0%\n",
      "home_improvement               6200         6200       100.0%\n",
      "home_services                  997          997        100.0%\n",
      "hospitality_services           1917         1917       100.0%\n",
      "hotels_lodging                 4602         4602       100.0%\n",
      "human_resources                1148         1148       100.0%\n",
      "import_export                  145          145        100.0%\n",
      "insurance_services             2207         2207       100.0%\n",
      "it_services                    3307         3307       100.0%\n",
      "jewelry_accessories            1527         1527       100.0%\n",
      "landscaping_garden             1201         1201       100.0%\n",
      "legal_services                 3289         3289       100.0%\n",
      "lifestyle_personal             877          877        100.0%\n",
      "manufacturing_general          1411         1411       100.0%\n",
      "marine_services                320          320        100.0%\n",
      "marketing_advertising          6806         6806       100.0%\n",
      "martial_arts                   237          237        100.0%\n",
      "media_publishing               2408         2408       100.0%\n",
      "medical_specialty              2571         2571       100.0%\n",
      "mental_health                  749          749        100.0%\n",
      "mining_extraction              342          342        100.0%\n",
      "music_industry                 7318         7318       100.0%\n",
      "nonprofit_charity              10134        10134      100.0%\n",
      "other_services                 2047         2047       100.0%\n",
      "outdoor_sports                 850          850        100.0%\n",
      "personal_professional          3763         3763       100.0%\n",
      "pet_services                   942          942        100.0%\n",
      "photography_video              4720         4720       100.0%\n",
      "printing_publishing            3787         3787       100.0%\n",
      "property_management            1109         1109       100.0%\n",
      "radio_podcast                  1057         1057       100.0%\n",
      "real_estate_commercial         1244         1244       100.0%\n",
      "real_estate_development        2142         2142       100.0%\n",
      "real_estate_residential        10825        10825      100.0%\n",
      "religious_faith                4001         4001       100.0%\n",
      "restaurant_dining              10963        10963      100.0%\n",
      "retail_general                 6503         6503       100.0%\n",
      "salon_spa                      2256         2256       100.0%\n",
      "security_safety                948          948        100.0%\n",
      "skateboarding_action           891          891        100.0%\n",
      "software_development           3926         3926       100.0%\n",
      "sports_recreation              11383        11383      100.0%\n",
      "sports_teams                   1301         1301       100.0%\n",
      "tattoo body_art                16           16         100.0%\n",
      "tattoo_body_art                177          177        100.0%\n",
      "tech_hardware                  838          838        100.0%\n",
      "telecommunications             2150         2150       100.0%\n",
      "textiles_manufacturing         1670         1670       100.0%\n",
      "tobacco_vaping                 253          253        100.0%\n",
      "training_development           1048         1048       100.0%\n",
      "transportation_services        1859         1859       100.0%\n",
      "travel_tourism                 4609         4609       100.0%\n",
      "unclassified                   73954        73954      100.0%\n",
      "vacation_recreation            1674         1674       100.0%\n",
      "vehicle_sales                  425          425        100.0%\n",
      "veterinary                     1006         1006       100.0%\n",
      "water_services                 550          550        100.0%\n",
      "web_digital                    5470         5470       100.0%\n",
      "wellness_fitness               4261         4261       100.0%\n",
      "wholesale_distribution         907          907        100.0%\n",
      "-----------------------------------------------------------------\n",
      "TOTAL                          352154       352154     100.0%\n",
      "\n",
      "Top 10 categories by extracted image count:\n",
      "   1. unclassified: 73,954 (21.0%)\n",
      "   2. sports_recreation: 11,383 (3.2%)\n",
      "   3. restaurant_dining: 10,963 (3.1%)\n",
      "   4. real_estate_residential: 10,825 (3.1%)\n",
      "   5. healthcare_general: 10,534 (3.0%)\n",
      "   6. design_creative: 10,366 (2.9%)\n",
      "   7. nonprofit_charity: 10,134 (2.9%)\n",
      "   8. fashion_apparel: 9,174 (2.6%)\n",
      "   9. education_k12: 8,572 (2.4%)\n",
      "  10. music_industry: 7,318 (2.1%)\n"
     ]
    }
   ],
   "source": [
    "# Show complete extraction results by category\n",
    "if extracted_images:\n",
    "    extracted_df = pd.DataFrame(extracted_images)\n",
    "    category_counts_extracted = extracted_df['category'].value_counts()\n",
    "    \n",
    "    print(\"\\nComplete extraction results by category:\")\n",
    "    print(f\"{'Category':<30} {'In Metadata':<12} {'Extracted':<10} {'%':<6}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    metadata_counts = sampled_df['category'].value_counts()\n",
    "    for category in sorted(metadata_counts.index):\n",
    "        in_metadata = metadata_counts[category]\n",
    "        extracted = category_counts_extracted.get(category, 0)\n",
    "        percentage = (extracted / in_metadata * 100) if in_metadata > 0 else 0\n",
    "        print(f\"{category[:30]:<30} {in_metadata:<12} {extracted:<10} {percentage:>5.1f}%\")\n",
    "\n",
    "    total_in_metadata = len(sampled_df)\n",
    "    total_extracted = len(extracted_images)\n",
    "    overall_percentage = (total_extracted / total_in_metadata * 100) if total_in_metadata > 0 else 0\n",
    "\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'TOTAL':<30} {total_in_metadata:<12} {total_extracted:<10} {overall_percentage:>5.1f}%\")\n",
    "    \n",
    "    # Show top categories by extracted count\n",
    "    print(f\"\\nTop 10 categories by extracted image count:\")\n",
    "    for i, (category, count) in enumerate(category_counts_extracted.head(10).items(), 1):\n",
    "        percentage = (count / total_extracted * 100)\n",
    "        print(f\"  {i:2d}. {category}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No images were extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373585fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved complete extraction metadata to: ..\\..\\output\\amazing_logos_v4\\data\\total_after_cleanup_metadata.csv\n",
      "\n",
      "Sample of extracted images:\n",
      "                      id            category                   filename  \\\n",
      "0  amazing_logo_v4000000      hotels_lodging  amazing_logo_v4000000.png   \n",
      "1  amazing_logo_v4000001  chemical_materials  amazing_logo_v4000001.png   \n",
      "2  amazing_logo_v4000003        unclassified  amazing_logo_v4000003.png   \n",
      "3  amazing_logo_v4000004          film_video  amazing_logo_v4000004.png   \n",
      "4  amazing_logo_v4000005        unclassified  amazing_logo_v4000005.png   \n",
      "\n",
      "   dataset_index  \n",
      "0              0  \n",
      "1              1  \n",
      "2              3  \n",
      "3              4  \n",
      "4              5  \n",
      "\n",
      "‚úÖ Saved complete extraction summary to: ..\\..\\output\\amazing_logos_v4\\images\\complete_extraction_summary.json\n",
      "\n",
      "Final Extraction Summary:\n",
      "  Total images in metadata9.csv: 352,154\n",
      "  Images successfully extracted: 352,154\n",
      "  Extraction success rate: 100.0%\n",
      "  Categories processed: 109\n",
      "  Failed extractions: 0\n",
      "  Output folder: ..\\..\\output\\amazing_logos_v4\\images\\total_after_cleanup\n",
      "\n",
      "üéØ Extraction complete! All images from metadata9.csv have been processed.\n",
      "Check the 'total_after_cleanup' folder for your extracted images.\n"
     ]
    }
   ],
   "source": [
    "# Save complete extraction metadata and summary\n",
    "if extracted_images:\n",
    "    extracted_df = pd.DataFrame(extracted_images)\n",
    "    extracted_df.to_csv(output_metadata_path, index=False)\n",
    "    print(f\"\\n‚úÖ Saved complete extraction metadata to: {output_metadata_path}\")\n",
    "    \n",
    "    # Show sample of extracted data\n",
    "    print(f\"\\nSample of extracted images:\")\n",
    "    print(extracted_df.head())\n",
    "    \n",
    "    # Save comprehensive summary statistics\n",
    "    category_distribution = extracted_df['category'].value_counts().to_dict()\n",
    "    \n",
    "    summary_stats = {\n",
    "        'extraction_type': 'complete_from_metadata9',\n",
    "        'total_images_in_metadata': len(sampled_df),\n",
    "        'total_images_extracted': len(extracted_images),\n",
    "        'extraction_success_rate': (len(extracted_images) / len(sampled_df) * 100) if len(sampled_df) > 0 else 0,\n",
    "        'categories_processed': len(category_distribution),\n",
    "        'failed_extractions': len(failed_extractions),\n",
    "        'image_size': IMAGE_SIZE,\n",
    "        'output_format': OUTPUT_FORMAT,\n",
    "        'output_folder': 'total_after_cleanup',\n",
    "        'category_distribution': category_distribution,\n",
    "        'source_metadata': 'metadata9.csv'\n",
    "    }\n",
    "    \n",
    "    summary_path = output_images_path.parent / 'complete_extraction_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved complete extraction summary to: {summary_path}\")\n",
    "    print(f\"\\nFinal Extraction Summary:\")\n",
    "    print(f\"  Total images in metadata9.csv: {summary_stats['total_images_in_metadata']:,}\")\n",
    "    print(f\"  Images successfully extracted: {summary_stats['total_images_extracted']:,}\")\n",
    "    print(f\"  Extraction success rate: {summary_stats['extraction_success_rate']:.1f}%\")\n",
    "    print(f\"  Categories processed: {summary_stats['categories_processed']}\")\n",
    "    print(f\"  Failed extractions: {summary_stats['failed_extractions']}\")\n",
    "    print(f\"  Output folder: {output_images_path}\")\n",
    "    \n",
    "    # Save failed extractions for debugging if any\n",
    "    if failed_extractions:\n",
    "        failed_df = pd.DataFrame(failed_extractions)\n",
    "        failed_path = output_images_path.parent / 'failed_extractions_complete.csv'\n",
    "        failed_df.to_csv(failed_path, index=False)\n",
    "        print(f\"  Failed extractions saved to: {failed_path}\")\n",
    "        \n",
    "        # Show failure analysis\n",
    "        failure_reasons = failed_df['reason'].value_counts()\n",
    "        print(f\"\\nFailure analysis:\")\n",
    "        for reason, count in failure_reasons.items():\n",
    "            print(f\"  {reason}: {count}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå No images were extracted!\")\n",
    "    if failed_extractions:\n",
    "        print(f\"Total failures: {len(failed_extractions)}\")\n",
    "        print(\"Sample failures:\")\n",
    "        for i, failure in enumerate(failed_extractions[:10], 1):\n",
    "            print(f\"  {i:2d}. {failure['id']}: {failure['reason']}\")\n",
    "            \n",
    "print(f\"\\nüéØ Extraction complete! All images from metadata9.csv have been processed.\")\n",
    "print(f\"Check the '{output_images_path.name}' folder for your extracted images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
